# af-private-scripts

This directory includes the scripts for gathering and analyzing the data used in the alevin-fry manuscript.


First of all, make usre you have enough available space on your disk. Then clone the repository and navigate to the main directory of the repo:
```
$git clone git@github.com:COMBINE-lab/af-private-scripts.git
$cd af-private-scripts
```

In order to run all the scripts in this repository, please make sure you have the binaries for running the fallowing tools.

> [salmon](https://github.com/COMBINE-lab/salmon)

> [alevinfry](https://github.com/COMBINE-lab/alevin-fry)

> [starsolo](https://github.com/alexdobin/STAR)

> [kb](https://github.com/pachterlab/kb_python)

### Gathering the references
First of all edit the config file `config.json` and put the relavent path for `kb`, `starsolo` and `salmon` in the config file. Also, set the `top_dir` to the directory where you wanna save all the files generated by these scritps.

Navigate to the `data_prep_scripts` directory and call the script for downloading the references.
```
$cd data_prep_scripts
$bash gather_refferences.sh config.json
```

If the script finishes running with no problem, you should have the fallowing files available in the `refs` direcotory:
```
refs/refdata-gex-GRCh38-2020-A/fasta/genome.fa
refs/refdata-gex-GRCh38-2020-A/genes/genes.gtf

refs/refdata-cellranger-GRCh38-3.0.0/fasta/genome.fa
refs/refdata-cellranger-GRCh38-3.0.0/genes/genes.gtf
refs/refdata-cellranger-GRCh38-3.0.0/transcriptome.fa

refs/refdata-gex-mm10-2020-A/fasta/genome.fa
refdata-gex-mm10-2020-A/genes/genes.gtf
refdata-gex-mm10-2020-A/geneid_to_name.txt

refs/refdata-cellranger-mm10-2.1.0/fasta/genome.fa
refs/refdata-cellranger-mm10-2.1.0/genes/genes.gtf
refs/refdata-cellranger-mm10-2.1.0/geneid_to_name.txt

refs/dr-101-cr-ref/fasta/genome.fa
refs/dr-101-cr-ref/genes/genes.gtf
refs/dr-101-cr-ref/geneid_to_name.txt
```

### Generating splici transcriptome for the alevin-fry pipeline
We need to build the splici which includes the intronic regions of the genes as well as the transcripts. To do so, please run the fallowing:
```
bash build_splici_txomes.sh config.json refs.csv
```

If this script runs successfully, you should be able to locate the fallowing filees:
```
refs/refdata-gex-GRCh38-2020-A/transcriptome_splici/transcriptome_splici_fl91_t2g.tsv
refs/refdata-gex-GRCh38-2020-A/transcriptome_splici/transcriptome_splici_fl91_t2g_3col.tsv
refs/refdata-gex-GRCh38-2020-A/transcriptome_splici/transcriptome_splici_fl91.fa

refs/refdata-cellranger-GRCh38-3.0.0/transcriptome_splici/transcriptome_splici_fl91_t2g.tsv
refs/refdata-cellranger-GRCh38-3.0.0/transcriptome_splici/transcriptome_splici_fl91_t2g_3col.tsv
refs/refdata-cellranger-GRCh38-3.0.0/transcriptome_splici/transcriptome_splici_fl91.fa

refs/refdata-cellranger-mm10-2.1.0/transcriptome_splici/transcriptome_splici_fl151_t2g.tsv
refs/refdata-cellranger-mm10-2.1.0/transcriptome_splici/transcriptome_splici_fl151_t2g_3col.tsv
refs/refdata-cellranger-mm10-2.1.0/transcriptome_splici/transcriptome_splici_fl151.fa

refs/refdata-gex-mm10-2020-A/transcriptome_splici/transcriptome_splici_fl150_t2g.tsv
refs/refdata-gex-mm10-2020-A/transcriptome_splici/transcriptome_splici_fl150_t2g_3col.tsv
refs/refdata-gex-mm10-2020-A/transcriptome_splici/transcriptome_splici_fl150.fa

refs/dr-101-cr-ref/transcriptome_splici/transcriptome_splici_fl98_t2g.tsv
refs/dr-101-cr-ref/transcriptome_splici/transcriptome_splici_fl98_t2g_3col.tsv
refs/dr-101-cr-ref/transcriptome_splici/transcriptome_splici_fl98.fa
```

### Building the indices

Run the `build_indices.sh` script for building the indices required for all the single cell processing of the mnuscript.
```
bash build_indices.sh config.sh refs.csv
```

Successfully running this step means that all the indices (salmon, starsolo and kb) required for the rest of the pipeline are available in the fallowing directories:
```
indices/human-cr3
indices/human-2020A
indices/mm10-2.1.0
indices/mm10-2020A
indices/dr-101
```

To download the prebuilt 10x permitlists for version 2 and version 3, run the `...` script. 

### Gathering the samples
To download all the experimental samples we used in the paper, please run:
```
./gather_samples.sh config.json
```
After the successful run of the script you should be able to find the fastq files at the fallowing directories:
```
samples/human-pbmc10k_v3_rl91
samples/dr_pineal_s2_rl98
samples/nucleus_mouse_placenta_E14.5_rl150
samples/velocity_mouse_pancreas_rl151
```

To generate the simulated dataset we used in the manuscritp, please fallow the instructions available at [STARsolo manuscript repository](https://github.com/dobinlab/STARsoloManuscript). Then place both the biological and technical reads in a directory called `samples/pbmc_5k_sims_human_CR_3.0.0_MultiGeneNo_rl91`.

### Running the Nextflow pipeline
Navigate to the `nf_pipeline` directory and edit the config file in order to run the nextflow pipeline. Also the download the Nextflow executable from [here](https://www.nextflow.io/).
```
$cd ../nf_pipeline
$curl -s https://get.nextflow.io | bash
```
The `nextflow` executable should be downloaded and placed in the working directory (`nf_pipeline`).
To configure the nextflow, open the `configs/main.config` and put the addresses for binaries of `salmon`, `alevinfry`, `star_solo` and `kb` in the lines 21 to 24. Also set the value of the `top.dir` parameter at line 16, to the same value you set for `top_dir` in the `config.json` file in the index building step. Also, set the value of `bench.dir` in line 15, to the address of the parent directory where you cloned the `af-private-scritps` repository.
Then run the nextflow pipeline by:
```
./launch.sh -n nextflow
```

This pipeline will perform single cell pre-processing with all three tools (`alevinfry`, `starsolo` and `kb`) for all five datasets. And the results for all the tools should be place at `nf_pipeline/output/`. Also the file `pipeline_trace.txt` should be generated in the `nf_pipeline` directory.

### Running the particular scripts for the simulated data
For the simualted data, we evaluate results with different modes of alevinfry, to do so, navigate to `simulated_specific` directory and execute the fallowing command:
```
bash salmon_af_sim.sh ../data_prep_scripts/config.json
```

### Computing the accuracy metrics for the simulated data

Navigate to `analysis-scripts` directory and run the `starsolo_sim_analysis.ipynb` jupyter notebook. You should add the path for the results of different tools in the `configs/ss_sim.json` file before running the notebook. After adding the paths, you can run the commands in the notebook to find the accuracy metrics.

### Post processing of the results

In this section we will show the pipelines of performing RNA velocity analysis for the mouse pancreas dataset, and clustering analysis for the mouse placenta dataset and the zebrafish pineal dataset. The results of all these analyses are detailly demonstrated in the manuscript.

The RNA velocity analysis is performed in python using [scVelo](https://scvelo.readthedocs.io/) on the quantification results of the pancreas dataset generated from the previous steps. in `analysis-scripts/mouse_pancreas_velocity` folder, We provide the ipynb scripts for analyzing the quanfitication results of STARsolo (`mouse_pancreas_st_velocity_analysis.ipynb`), kallisto|bustools (mouse_pancreas_kb_velocity_analysis.ipynb) and alevin-fry (`mouse_pancreas_af_velocity_analysis.ipynb`) separately. To run the scripts, you need to prepare the path to the quantification results and the path to the TSV file containing ensembl IDs and the corresponding gene names. We encourage you to run the script interactively and explore the results after assigning ambiguous counts differently, so as to better understand the effects of ambiguous counts assigning strategies. Notice that the scripts involves some matrix operations for large matrices, so please make sure you have enough RAM before running them (it can take over 32 GB RAM). 

The clustering analyses, however, are performed in R with the help of [Seurat 4.0](https://satijalab.org/seurat/). The scripts are under `analysis-scripts/mouse_placenta_clustering` for the mouse placenta dataset and `analysis-scripts/zebrafish_pineal_expression` for the zebrafish pineal dataset. Again, you need to provide the paths to the quanfication results, the TSV file containing ensembl IDs and the corresponding gene names. For the mouse placenta dataset, you need also provide the implemented `cellRangerLikeEmptyDrops.R` script, which is included in the `analysis-scripts/mouse_placenta_clustering` folder. Although after providing the paths, the code can be run without any manual operation, we still recommend you to run the scripts interactively to explore the properties of the Seurat objects after each step. In the `analysis-outputs` folder, we provided the rendered HTMLs of the clustering analysis results of the two datasets. 

### Comparing the performance of different tools
To generated the timing and peak memory plots, open the jupyter notebook at `plot_scripts/plot-time-rss.ipynb`, and add the address for `pipeline_trace.txt` generated by the nextflow pipeline to line 3. Then, execute all the commands in the notebook in order.

